{"cells":[{"cell_type":"markdown","metadata":{"id":"f9b1e7e1"},"source":["<br>\n","\n","# <center> Sentiment Analysis of Twitter Tweets using NLP and LSTM\n","\n","## <center> Pridiction on Real Time Text"],"id":"f9b1e7e1"},{"cell_type":"markdown","metadata":{"id":"fMmkZrOJ_pav"},"source":["<br>\n","\n","---\n","\n","<br>\n"],"id":"fMmkZrOJ_pav"},{"cell_type":"markdown","source":["# List of Contents\n","\n","\n","\n","*   [1. Initialization](#initialization)\n","*   [2. Loading the required Data](#loading-the-required-data)\n","*   [3. Text Processing](#text-preprocessing)\n","*   [4. Prediction](#prediction)\n","\n","\n","<br>\n"],"metadata":{"id":"0lPYGYRviSFH"},"id":"0lPYGYRviSFH"},{"cell_type":"markdown","source":["<br>\n","<br>\n","\n","<a name='initialization'></a>\n","# 1. Initialization"],"metadata":{"id":"XlEDoO_Qh-4J"},"id":"XlEDoO_Qh-4J"},{"cell_type":"markdown","source":["<br>\n","\n","## 1.1. Colab Configuration"],"metadata":{"id":"p1gpbaYGHLB4"},"id":"p1gpbaYGHLB4"},{"cell_type":"markdown","source":["### 1.1.1. Mount Google Drive"],"metadata":{"id":"0A2hpwIu9JIJ"},"id":"0A2hpwIu9JIJ"},{"cell_type":"code","source":["'''\n","    This is required if the code runs in Google Colab.\n","    - this code will mount Google Drive for Colab.\n","    - the code needs to run only once.\n","'''\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"O4_sQAVw83Cx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680736374381,"user_tz":-60,"elapsed":2730,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}},"outputId":"ef58ef26-2156-44e5-fe0d-6ff7fe6663dd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"id":"O4_sQAVw83Cx"},{"cell_type":"markdown","source":["<br>\n","\n","### 1.1.2. Defining Root Directory"],"metadata":{"id":"L-ETilP4FEhn"},"id":"L-ETilP4FEhn"},{"cell_type":"code","source":["# -----------------------------------------------\n","#   Check the code is running on Colab or not   |\n","# -----------------------------------------------\n","import sys\n","is_running_on_colab = 'google.colab' in sys.modules\n","\n","\n","# -----------------------------------------------\n","#               Root Directory                  |\n","# -----------------------------------------------\n","# this directory will be used as Root Directory to read/write any file\n","if is_running_on_colab:\n","    # for google-colab\n","    rootDir = '/content/drive/MyDrive/_ML/Twitter Sentiment Analysis LSTM'\n","else:\n","    # for application\n","    rootDir = './mlData/'\n","    "],"metadata":{"id":"Dq3G16RuFMbG","executionInfo":{"status":"ok","timestamp":1680736374382,"user_tz":-60,"elapsed":28,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":2,"outputs":[],"id":"Dq3G16RuFMbG"},{"cell_type":"markdown","metadata":{"id":"2f142dde"},"source":["<br>\n","\n","## 1.2. Import Libraries"],"id":"2f142dde"},{"cell_type":"code","execution_count":3,"metadata":{"id":"4677354a","executionInfo":{"status":"ok","timestamp":1680736375516,"user_tz":-60,"elapsed":1160,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"outputs":[],"source":["# importing all the required libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import json"],"id":"4677354a"},{"cell_type":"markdown","source":["<br>\n","\n","<a name='loading-the-required-data'></a>\n","# 2. Loading the required Data"],"metadata":{"id":"sWea6aloLvhY"},"id":"sWea6aloLvhY"},{"cell_type":"markdown","source":["<br>\n","\n","> Loading the Model"],"metadata":{"id":"tuy7dtHXP-uF"},"id":"tuy7dtHXP-uF"},{"cell_type":"code","source":["# importing the library\n","from keras.models import load_model\n","\n","# defining the name of the model\n","modelName = 'Sentiment_Analyser.h5'\n","\n","# creating the path\n","path = f\"{rootDir}/03. Generated Data/{modelName}\"\n","\n","# Load the model from the file\n","model = load_model( filepath = path )"],"metadata":{"id":"AMGcUIAiL0W4","executionInfo":{"status":"ok","timestamp":1680736383766,"user_tz":-60,"elapsed":8253,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"id":"AMGcUIAiL0W4","execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","> Loding the Tokenizer, Config, and Target_names"],"metadata":{"id":"elCJeuwhQChc"},"id":"elCJeuwhQChc"},{"cell_type":"code","source":["import pickle\n","\n","filename = 'data.pkl'\n","path = f'{rootDir}/03. Generated Data/{filename}'\n","\n","# open a file for writing in binary mode\n","with open(path, 'rb') as f:\n","    # deserialize and load the file\n","    data = pickle.load(f)"],"metadata":{"id":"snVO-sqjQLpd","executionInfo":{"status":"ok","timestamp":1680736383768,"user_tz":-60,"elapsed":39,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"id":"snVO-sqjQLpd","execution_count":5,"outputs":[]},{"cell_type":"code","source":["tokenizer = data['tokenizer']\n","config = data['config']\n","target_names = data['target_names']"],"metadata":{"id":"ifOvyZ88UCkw","executionInfo":{"status":"ok","timestamp":1680736383770,"user_tz":-60,"elapsed":38,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"id":"ifOvyZ88UCkw","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","<br>\n","\n","<a name='text-preprocessing'></a>\n","\n","# 3. Text Processing"],"metadata":{"id":"7GxlveMIgMKj"},"id":"7GxlveMIgMKj"},{"cell_type":"markdown","source":["## 3.1. Filtering the text"],"metadata":{"id":"kHG1o0YsMh3w"},"id":"kHG1o0YsMh3w"},{"cell_type":"code","source":["# importing regex library\n","import re   \n","\n","# defining the function to filter tweets\n","def filter_text(\n","        text: str\n","    ):\n","\n","    '''\n","        Filtering the tweet string to extract meaningful text.\n","        \n","\n","        Parameter\n","        ---------\n","        text\n","            a text (string)\n","\n","        Return\n","        ------\n","        ret\n","            filtered tweet string\n","    '''\n","\n","\n","    # 01. converting the text to lower case\n","    text = text.lower()\n","\n","    # 02. filtering non-letters from the text so only valid words remain\n","    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n","\n","    # 03. removes a specific word from text if exists\n","    word_to_remove = 'rt' # defining the word to remove\n","    # text = re.sub(r'\\b' + word_to_remove + r'\\b', '', text)\n","\n","    # 04. striping the white space at starting or end of the text\n","    text = text.strip()\n","\n","\n","    # returning the filtered text\n","    return text"],"metadata":{"id":"saIzBrAtcc8x","executionInfo":{"status":"ok","timestamp":1680736383772,"user_tz":-60,"elapsed":39,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":7,"outputs":[],"id":"saIzBrAtcc8x"},{"cell_type":"markdown","source":["<br>\n","\n","## 3.2. NLP"],"metadata":{"id":"EQsC-z16GqEX"},"id":"EQsC-z16GqEX"},{"cell_type":"markdown","source":["<br>\n","\n","### 3.2.1. Initializing NLTK"],"metadata":{"id":"ZrWIeieNHcMI"},"id":"ZrWIeieNHcMI"},{"cell_type":"code","source":["import nltk\n","\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGyCpEDcHH23","executionInfo":{"status":"ok","timestamp":1680736384692,"user_tz":-60,"elapsed":958,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}},"outputId":"0fb71a8d-561e-4fb1-9ddd-3805b2eb4ac8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}],"id":"EGyCpEDcHH23"},{"cell_type":"markdown","source":["<br>\n","\n","### 3.2.2. Lemmatizer Design"],"metadata":{"id":"He-UXWAAHSa9"},"id":"He-UXWAAHSa9"},{"cell_type":"markdown","source":["<br>\n","\n","> Designing Parts-Of-Speech Tagger"],"metadata":{"id":"w0OShA0wKFws"},"id":"w0OShA0wKFws"},{"cell_type":"code","source":["# accessing the wordnet library for parts-of-speech tagging\n","from nltk.corpus import wordnet\n","\n","# defining function for POS tagging of a word\n","def get_wordnet_pos(word):\n","    '''\n","        Map POS tag to first character the WordNetLemmatizer() function accepts\n","\n","        Parameters\n","        ----------\n","        word\n","            the word whose Parts-Of-Speech will be tagged\n","\n","        Return\n","        ------\n","        ret\n","            tagged Parts-Of-Speech of the word\n","    '''\n","    \n","    # accessing the first alphabet\n","    tag = nltk.pos_tag([word])[0][1][0].lower()\n","    \n","    # definning the dictionary\n","    tag_dict = {\n","        \"a\": wordnet.ADJ,\n","        \"n\": wordnet.NOUN,\n","        \"v\": wordnet.VERB,\n","        \"r\": wordnet.ADV\n","    }\n","    \n","    # getting the tagged parts-of-speech\n","    pos = tag_dict.get(tag, wordnet.NOUN)\n","    \n","    # returning the tagged pos\n","    return pos"],"metadata":{"id":"v3ARS1r6J_w9","executionInfo":{"status":"ok","timestamp":1680736384694,"user_tz":-60,"elapsed":23,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":9,"outputs":[],"id":"v3ARS1r6J_w9"},{"cell_type":"markdown","source":["<br>\n","\n","> Desiging Lemmatizer"],"metadata":{"id":"6SRPaGtjLT-J"},"id":"6SRPaGtjLT-J"},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","\n","def _lemmatize(sentence):\n","    '''\n","        Lemmatize the words of a sentence.\n","\n","        Parameters\n","        ----------\n","        sentence\n","        the sentence whose words will be lemmatized\n","\n","        Return\n","        ------\n","        ret\n","        lemmatized sentence\n","    '''\n","        \n","    # creating the lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","\n","    # definign list to store the lemmatized words\n","    lemmatized_words = []\n","        \n","    # looping through the words of the sentence to lemmatize\n","    for word in sentence.split():\n","        # parts-of-speech of the word\n","        pos = get_wordnet_pos(word)\n","        \n","        # lemmatized word\n","        lemma = lemmatizer.lemmatize(word=word, pos=pos)\n","        \n","        # appending the lemmatized word to the list\n","        lemmatized_words.append(lemma.lower())\n","\n","    # creating the sentence with lemmatized words\n","    lemmatized_sentence = ' '.join(lemmatized_words)\n","\n","    # returning the lemmatized sentence\n","    return lemmatized_sentence"],"metadata":{"id":"_YrbyXjDHKJ6","executionInfo":{"status":"ok","timestamp":1680736384695,"user_tz":-60,"elapsed":22,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":10,"outputs":[],"id":"_YrbyXjDHKJ6"},{"cell_type":"markdown","source":["<br>\n","\n","## 3.3. Tokenizing & Padding"],"metadata":{"id":"GXhhOQmnPXOC"},"id":"GXhhOQmnPXOC"},{"cell_type":"code","source":["# accessing the library for tokenizing and padding\n","from keras.utils import pad_sequences"],"metadata":{"id":"GCs5wXesPV_K","executionInfo":{"status":"ok","timestamp":1680736384696,"user_tz":-60,"elapsed":16,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":11,"outputs":[],"id":"GCs5wXesPV_K"},{"cell_type":"markdown","metadata":{"id":"JWHlXpwuiLn1"},"source":["<br>\n","<br>\n","\n","<a name='prediction'></a>\n","\n","# 4. Prediction"],"id":"JWHlXpwuiLn1"},{"cell_type":"code","source":["def get_prediction(\n","        text: str\n","    ):\n","\n","    '''\n","        This function takes a Text as String and Predicts the Class Name\n","\n","        Parameter\n","        ---------\n","        text\n","            a text (string)\n","\n","        Return\n","        ------\n","        ret\n","            predicted class name (string)\n","    '''\n","\n","    # filtering the text\n","    filtered_text = filter_text(text)\n","\n","    # lemmatizing the filtered text\n","    lemma = _lemmatize(filtered_text)\n","\n","    # transforming the lemma to a sequence of integers\n","    tokens = tokenizer.texts_to_sequences([lemma])\n","\n","    # padding the tokens to the a defined length\n","    sequence = pad_sequences(tokens, padding='post', maxlen=config['max_length'])\n","\n","    # predicting result for the sequence\n","    predicted_class = model.predict(sequence, verbose=0)\n","\n","    # extracting the index of the predicted class with maximum probability\n","    predicted_class = int( np.argmax(predicted_class, axis=1) )\n","\n","    # extracting the class name\n","    predicted_class_name = target_names[predicted_class]\n","\n","    # returning the predicted class name\n","    return predicted_class_name\n"],"metadata":{"id":"YPi1-vfBLmLG","executionInfo":{"status":"ok","timestamp":1680736384697,"user_tz":-60,"elapsed":16,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"id":"YPi1-vfBLmLG","execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_ndIKLMrK4z","executionInfo":{"status":"ok","timestamp":1680736391352,"user_tz":-60,"elapsed":6668,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}},"outputId":"89f842a8-aace-46eb-b69e-783374724eb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Type Something: I had a wonderful day today!\n","\n"," Pricted Sentiment : Positive\n","\n","'I had a wonderful day today!' has Positive sentiment\n"]}],"source":["# taking a input as text\n","# print('Type Something: \\n')\n","text = input('Type Something: ')\n","\n","# predicting on the text\n","predicted_class_name = get_prediction(text)\n","predicted_sentiment = predicted_class_name.capitalize()\n","\n","# displaying the predicted result\n","print(f'\\n Pricted Sentiment : {predicted_sentiment}')\n","print(f\"\\n'{text}' has {predicted_sentiment} sentiment\")\n"],"id":"p_ndIKLMrK4z"},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":41,"status":"ok","timestamp":1680736391354,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"},"user_tz":-60},"id":"IfYAelXbszc3"},"outputs":[],"source":[],"id":"IfYAelXbszc3"}],"metadata":{"colab":{"collapsed_sections":["XlEDoO_Qh-4J","0A2hpwIu9JIJ","L-ETilP4FEhn","2f142dde","kHG1o0YsMh3w","EQsC-z16GqEX","ZrWIeieNHcMI"],"provenance":[{"file_id":"1ohgWI-XVrZgze8gwjwA9pFmxBtzbBYvq","timestamp":1657829296988}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}